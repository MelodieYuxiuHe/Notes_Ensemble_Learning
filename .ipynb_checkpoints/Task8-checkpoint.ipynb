{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different from voting, bagging method is not a simply selection among models, it change the training process to bring differences among models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling from the training set (with putting it back). T sample set with each has sample size K. We train each model using one of the T sample set, then ensemble all the models.\n",
    "\n",
    "Because we train the model on slightly different training dataset, there are differences among models, which is good for ensemble learning.\n",
    "\n",
    "Also, bagging can reduce the variance, thus work well on decision tree (without pruning) and neural network that can be easily impacted by the sample change. Bagging also works well on high dimension small sample dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn: BaggingRegressor and BaggingClassifier (tree model as default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Info Entropy: \n",
    "The reason of choosing decition tree is to select features to make a 'pure' classification, thus reduce the info entropy\n",
    "\n",
    "Info Gain:\n",
    "larger info gain means more 'variant' data can be classfied upon this node, resulting a 'purer' classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree is the process of recursively constructing binary trees. \n",
    "For regresion decision tree, use least mean square error.\n",
    "For classification decision tree, use Gini index to select features and generate binary tree.\n",
    "\n",
    "We want to minimize the gini index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gain Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gain ( B, A ) / Entropy (B | A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest is a bagging of decision trees. Training set and features for each tree is generated from random sampling. Random forest prediction is from different tree model prediction using voting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting just take the result and vote (all models train on same training set)\n",
    "Bagging change the training set using random sampling\n",
    "\n",
    "Both method reduce the variance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
