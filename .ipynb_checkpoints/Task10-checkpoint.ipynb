{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Stagewise Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost and Forward Stagewise Algorithm\n",
    "\n",
    "Adaboost is a special case of forward stagewise algorithm. \n",
    "Adaboost: addition model, Exponential loss function, Forward Stagewise learning algorithm, binary classification\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aditive model\n",
    "\n",
    "$$f(x) = \\sum_{m=1}^{M} \\beta_{m}b(x; \\gamma_{m})$$\n",
    "\n",
    "weight: $$\\beta_{m}$$\n",
    "\n",
    "base function: $$b(x; \\gamma_{m})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Based on residual \n",
    "- Base function: Tree\n",
    "- GBDT has a depth larger than 1 (Adaboost has depth 1)\n",
    "- Generally, GBDT has a max number of leaves between 8 and 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step1: Calculate the average of the target label\n",
    "\n",
    "calculate the average of Y\n",
    "\n",
    "### Step2: Calculate the residuals\n",
    "\n",
    "actual value - predicted value\n",
    "\n",
    "### Step3: Construct a decision Tree\n",
    "\n",
    "use the residual to construct\n",
    "(if there are more than one sample (residual) in a classification category, take the average)\n",
    "\n",
    "### Step4: Predict target label using all the trees within ensemble\n",
    "\n",
    "We set up learning rate to avoid overfitting and thus use more decision trees to 'optimize' and get close to the solution.\n",
    "\n",
    "\n",
    "### Step5: Compute residuals\n",
    "\n",
    "### Step6: Repeat step 3-5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimentation that takes a small incremental improvement towards the goal generally achieve a lower variance and a comparable bias. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.009154859960319"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,\n",
    "max_depth=1, random_state=0, loss='ls').fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43914297532066326"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = make_regression(random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, y, random_state=0)\n",
    "reg = GradientBoostingRegressor(random_state=0)\n",
    "reg.fit(X_train, y_train)\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on GradientBoostingRegressor and GradientBoostingClassifier Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoostingRegressor Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss: Default ls. ls (least squares regression), lad (least absolute deviation), huber (combination of ls and lad), quantile (allows quantile regression)\n",
    "\n",
    "learning_rate: default=0.1\n",
    "\n",
    "n_estimators: int, default=100, # of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.\n",
    "\n",
    "criterion: {‘friedman_mse’, ‘mse’, ‘mae’}, default=’friedman_mse’\n",
    "\n",
    "subsample: default=1.0\n",
    "\n",
    "min_samples_split\n",
    "\n",
    "min_samples_leaf\n",
    "\n",
    "min_weight_fraction_leaf\n",
    "\n",
    "max_depth\n",
    "\n",
    "min_impurity_decrease\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoostingRegressor Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature_importances_\n",
    "\n",
    "oob_improvement_\n",
    "\n",
    "train_score_\n",
    "\n",
    "loss_\n",
    "\n",
    "init_\n",
    "\n",
    "estimators_\n",
    "\n",
    "n_classes_int: The number of classes, set to 1 for regressors.\n",
    "\n",
    "n_estimators_\n",
    "\n",
    "n_features_\n",
    "\n",
    "max_features_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoostingClassifier Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no criterion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
